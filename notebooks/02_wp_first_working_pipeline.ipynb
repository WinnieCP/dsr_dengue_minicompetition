{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653d4930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../scr')\n",
    "from transformers import SelectFeatures, Impute, select_city, create_lagged_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359108d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f329c216",
   "metadata": {},
   "source": [
    "### Load the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9725765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/dengue_features_train.csv',\n",
    "                             index_col=[0,1,2])\n",
    "train_labels = pd.read_csv('../data/dengue_labels_train.csv',\n",
    "                           index_col=[0,1,2])\n",
    "test_features = pd.read_csv('../data/dengue_features_test.csv',\n",
    "                             index_col=[0,1,2])\n",
    "\n",
    "train_features = create_lagged_column(train_features,'station_precip_mm',lag=4)\n",
    "train_features = create_lagged_column(train_features,'station_avg_temp_c',lag=18)\n",
    "\n",
    "test_features = create_lagged_column(test_features,'station_precip_mm',lag=4)\n",
    "test_features = create_lagged_column(test_features,'station_avg_temp_c',lag=18)\n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de784a01",
   "metadata": {},
   "source": [
    "### Splitting the provided training data by city and into train and validation data for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718a354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dictionaries to hold the train and validation data for each city\n",
    "full_train_X = {}\n",
    "test_X = {}\n",
    "train_X = {}\n",
    "validate_X = {}\n",
    "full_train_y = {}\n",
    "train_y = {}\n",
    "validate_y = {}\n",
    "cities = ['sj', 'iq']\n",
    "city_names = {'sj':'San Juan, Puerto Rico', 'iq':'Iquitos, Peru' }\n",
    "\n",
    "# make the train validation split\n",
    "for city in cities:\n",
    "    full_train_X [city] = select_city(train_features, city = city)\n",
    "    full_train_y [city] = select_city(train_labels, city = city)\n",
    "    test_X [city] = select_city(test_features, city = city)\n",
    "    \n",
    "    assert len(full_train_X [city]) == len(full_train_y [city])\n",
    "    \n",
    "    split_idx = int(len(full_train_X[city])*0.8)\n",
    "    \n",
    "    train_X [city] = full_train_X [city].head(split_idx)\n",
    "    train_y [city] = full_train_y [city].head(split_idx)\n",
    "    \n",
    "    validate_X [city] = full_train_X [city].tail(len(full_train_X[city]) - split_idx)\n",
    "    validate_y [city] = full_train_y [city].tail(len(full_train_y[city]) - split_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01db235",
   "metadata": {},
   "source": [
    "Check that the split worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee45037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- San Juan, Puerto Rico -----------------\n",
      "training\n",
      "X (748, 23) y (748, 1)\n",
      "validation\n",
      "X (188, 23) y (188, 1)\n",
      "all\n",
      "X (936, 23) y (936, 1) \n",
      "\n",
      "------------- Iquitos, Peru -----------------\n",
      "training\n",
      "X (416, 23) y (416, 1)\n",
      "validation\n",
      "X (104, 23) y (104, 1)\n",
      "all\n",
      "X (520, 23) y (520, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for city in cities:\n",
    "    print('------------- '+city_names[city]+' -----------------')\n",
    "    print(\"training\")\n",
    "    print('X', train_X[city].shape, 'y', train_y[city].shape)\n",
    "    print('validation')\n",
    "    print('X', validate_X[city].shape, 'y', validate_y[city].shape)\n",
    "    print('all')\n",
    "    print('X', full_train_X[city].shape, 'y', full_train_y[city].shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dccce5",
   "metadata": {},
   "source": [
    "### Build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35ad0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the general setup of the pipeline\n",
    "\n",
    "\n",
    "# I added the SimpleImputer, because the ffill method fails when there is a nan value in the first row,\n",
    "# the first row is filled with the median in that case\n",
    "\n",
    "# define parameters for the different transformers\n",
    "\n",
    "myfeatures = ['reanalysis_specific_humidity_g_per_kg', \n",
    "                 'reanalysis_dew_point_temp_k', \n",
    "                 'station_avg_temp_c', \n",
    "                 'station_min_temp_c',\n",
    "             'station_precip_mm_lagged_4.00',\n",
    "             'station_avg_temp_c_lagged_18.00']\n",
    "\n",
    "pipe ={}\n",
    "for city in cities:\n",
    "    pipe[city] = Pipeline(steps=[('feature_selection', SelectFeatures), ('ffill_nans', Impute),\n",
    "                       ('median_nans', SimpleImputer(strategy='median')),('scaler',RobustScaler())])\n",
    "#('poly',PolynomialFeatures(degree=2))\n",
    "#myfeatures = [x for x in train_X['sj'].columns if x!='week_start_date']\n",
    "\n",
    "# Here, we set the parameters for all the transformers that we use in the pipeline\n",
    "\n",
    "this_pipe = {}\n",
    "for city in cities:\n",
    "    this_pipe[city]=pipe[city].set_params(feature_selection__kw_args={'features':myfeatures})\n",
    "    # Here, we add whatever model we want to use as a last step in the pipeline\n",
    "    #this_pipe[city].steps.append(('lin_reg', Ridge(alpha=.1)))\n",
    "    this_pipe[city].steps.append(('xgb',xgb.XGBRegressor(max_depth=4)))\n",
    "\n",
    "\n",
    "#this_pipe.steps.append(('lin_reg', ElasticNet(alpha=.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3496839",
   "metadata": {},
   "source": [
    "### Training one model per city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec469803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train one model per city and save them in this dictionary\n",
    "model = {}\n",
    "for city in cities:\n",
    "    model[city] = this_pipe[city].fit(train_X[city], train_y[city])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5ba4d",
   "metadata": {},
   "source": [
    "### Predicting on training and validation data set for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff862762",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = {}\n",
    "pred_valid = {}\n",
    "\n",
    "for city in cities:\n",
    "    pred_train[city] = model[city].predict(train_X[city])\n",
    "    pred_valid[city] = model[city].predict(validate_X[city])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0d6d7",
   "metadata": {},
   "source": [
    "### Quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5dd130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MEAN ABSOLUTE ERROR\\n')\n",
    "for city in cities:\n",
    "    print('------ '+city_names[city]+' ---------')\n",
    "    print(f'Training:   {mean_absolute_error(train_y[city], pred_train[city]):1.4f}')\n",
    "    print(f'Validation: {mean_absolute_error(validate_y[city], pred_valid[city]):1.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5a062",
   "metadata": {},
   "source": [
    "with lag 18 and poly degree 2\n",
    "\n",
    "MEAN ABSOLUTE ERROR\n",
    "\n",
    "------ San Juan, Puerto Rico ---------\n",
    "Training:   11.9508\n",
    "Validation: 24.9103\n",
    "\n",
    "------ Iquitos, Peru ---------\n",
    "Training:   1.9890\n",
    "Validation: 8.1461\n",
    "\n",
    "\n",
    "with lag 20 in temp\n",
    "\n",
    "MEAN ABSOLUTE ERROR\n",
    "\n",
    "------ San Juan, Puerto Rico ---------\n",
    "Training:   15.6495\n",
    "Validation: 25.0402\n",
    "\n",
    "------ Iquitos, Peru ---------\n",
    "Training:   2.6335\n",
    "Validation: 8.3078\n",
    "\n",
    "\n",
    "without lags\n",
    "\n",
    "MEAN ABSOLUTE ERROR\n",
    "\n",
    "------ San Juan, Puerto Rico ---------\n",
    "Training:   29.1376\n",
    "Validation: 24.7177\n",
    "\n",
    "------ Iquitos, Peru ---------\n",
    "Training:   6.1009\n",
    "Validation: 6.9307\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03810773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe here, we could save the pipeline and the errors and hyper parameters (and later also a plot)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1449a1",
   "metadata": {},
   "source": [
    "### Plot of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d981bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention, attention, this really depends on pandas and everyone not mixing up the rows\n",
    "\n",
    "training_results = {}\n",
    "validation_results = {}\n",
    "\n",
    "# setting up figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, gridspec_kw = {'width_ratios':[2.5, 1]})\n",
    "fig.set_size_inches(14,8)  \n",
    "axes = axes.flatten()\n",
    "for i in range(4):\n",
    "    axes[i].spines['top'].set_visible(False)\n",
    "    if i%2:\n",
    "        axes[i].yaxis.tick_right()\n",
    "        axes[i].spines['left'].set_visible(False)\n",
    "        axes[i].yaxis.set_label_position(\"right\")\n",
    "    else:\n",
    "        axes[i].spines['right'].set_visible(False)        \n",
    "        \n",
    "plt.subplots_adjust(hspace = 0.75, wspace = 0.08)\n",
    "\n",
    "# combine the prediction and true values in one dataframe\n",
    "for city in cities:\n",
    "    training_results[city] = train_y[city]\n",
    "    training_results[city]['pred'] = pred_train[city]\n",
    "    validation_results[city] = validate_y[city]\n",
    "    validation_results[city]['pred'] = pred_valid[city]\n",
    "\n",
    "# plot the predictions and true values for each city and each data set (training/validation)\n",
    "for i, city in enumerate(cities):\n",
    "    training_results[city].plot(y = ['total_cases', 'pred'], ax = axes[2*i], \n",
    "                                label = ['True', 'Prediction'], color = ['.75','orangered'])\n",
    "    validation_results[city].plot(y = ['total_cases','pred'], ax = axes[2*i+1], \n",
    "                                  label = ['True','Prediction'], color = ['.75','b'])\n",
    "    axes[2*i].set_title(city_names[city])\n",
    "    axes[2*i].set_ylim(0,full_train_y[city].total_cases.max())\n",
    "    axes[2*i+1].set_ylim(0,full_train_y[city].total_cases.max())\n",
    "    \n",
    "# format axes\n",
    "legend_title=['Training data','Validation data']*2\n",
    "for i in range(4):\n",
    "    axes[i].set_ylabel('cases')\n",
    "    axes[i].set_xlabel('time (year, week)')\n",
    "    axes[i].legend(frameon=False, title = legend_title[i], loc = i%2+1)\n",
    "    axes[i].tick_params(axis='x', labelrotation=45)\n",
    "    axes[i].set_facecolor('.98')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6c38d",
   "metadata": {},
   "source": [
    "### Predict the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = {}\n",
    "for city in cities:\n",
    "    pred_test[city] = model[city].predict(test_X[city])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7947c",
   "metadata": {},
   "source": [
    "### Get the prediction into the right format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for city in cities:\n",
    "    df_hlp = test_X[city].copy()\n",
    "    df_hlp['total_cases'] = pred_test[city]\n",
    "    df_hlp = pd.concat([df_hlp], keys=[city], names=['city'])\n",
    "    results.append(pd.DataFrame(df_hlp.total_cases.astype(int)))\n",
    "final = pd.concat(results)\n",
    "final.to_csv('../predictions/prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12d83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7053abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsr-trees",
   "language": "python",
   "name": "dsr-trees"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
